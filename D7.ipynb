{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Kaggle-project : Natural Language Processing with Disaster Tweets","metadata":{}},{"cell_type":"markdown","source":"### 1. Importing ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\n#import torch\nfrom transformers import TFBertForSequenceClassification\n#from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Loading the data ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/natural-language-processing-with-disaster-tweets/kaggle nlp/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/natural-language-processing-with-disaster-tweets/kaggle nlp/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/natural-language-processing-with-disaster-tweets/kaggle nlp/sample_submission.csv\")\nprint(\"Train shape : \",train.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()\nprint(test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Data Cleaning","metadata":{}},{"cell_type":"code","source":"#missing values\nfor col in train:\n  is_null = train[col].isnull().sum()\n  print(col + \":\" + str(is_null))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import html\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnew_train = train.drop_duplicates('text',keep='first')\nnew_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install demoji","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(new_train.shape)\nprint(train.shape)\nimport demoji\nimport emoji\nfor row in new_train['text']:\n    #print(row)\n    row=re.sub(r'https*\\S+', ' ', row) # removing only links\n    #row = re.sub(r\"(@[A-Za-z0â€“9_]+)|[^\\w\\s]|#|http\\S+\", \"\", row) # removing links and hash characters\n    #row = emoji.demojize(row, delimiters=(\"\", \"\"))\n    #print(row)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Splitting the data","metadata":{}},{"cell_type":"code","source":"train_sentences = new_train.text.values # for cleaned data\nlabels = new_train.target.values #for cleaned data\n#train_sentences = train.text.values\n#labels = train.target.values\nlen(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_input,val_input,train_label,val_label= train_test_split(train_sentences,labels,test_size=0.1, random_state=38)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. BERT Tokens, paddings and masks","metadata":{}},{"cell_type":"code","source":"#do_encode does following:\n#1. splits setence into tokens.\n#2. adds '[CLS]' and '[SEP]'\n#3. maps tokens to id's\n#4. adds paddings\n#5. creates attention masks\ndef do_encode(sentences,maximum):\n    ids =[]\n    attention_masks = []\n    for sentence in sentences:\n        encoded = tokenizer.encode_plus(sentence,add_special_tokens = True, max_length = maximum,\n                                       pad_to_max_length = True,\n                                       return_attention_mask = True)\n        ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    \n    ids = tf.convert_to_tensor(ids)\n    attention_masks = tf.convert_to_tensor(attention_masks)\n    return ids,attention_masks\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Finding max length as it is required for BERT padding\ntrain_max = max([len(sen) for sen in train_input])\nval_max = max([len(sen) for sen in val_input])\nprint(train_max)\nprint(val_max)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding train data\ntrain_ids,train_masks =do_encode(train_input,train_max) # we will now encode train text values\nprint(train_ids.shape)\nprint(train_masks.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Encoding validation data\nval_ids,val_masks = do_encode(val_input,val_max)\nprint(val_ids.shape)\nprint(val_masks.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Training the model","metadata":{}},{"cell_type":"code","source":"#Converting to tensor objects\ntrain_label = tf.convert_to_tensor(train_label)\nval_label = tf.convert_to_tensor(val_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We are using pretrained BertForSequenceClassification model\nbert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Code for setup from here: https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c\noutput_dir=\"./result\"\nmodel_save = \"./model/\"\n# callbacks are not necessary but will improve the results\ncallbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save,\n                                               save_weights_only=True,\n                                               monitor='val_loss',\n                                               mode='min',\n                                               save_best_only = True),keras.callbacks.TensorBoard(log_dir=output_dir)]\n\nprint(bert_model.summary())\n\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n\n#compiling the model - configures the model for  training\nbert_model.compile(loss=loss,optimizer = optimizer,metrics=[metric])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trains the model for fixed amount of epochs(dataset iterations) - in our case 4 epochs\nhistory = bert_model.fit([train_ids,train_masks],train_label,batch_size=32,epochs=4,validation_data=([val_ids,val_masks],val_label),callbacks=callbacks)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. Results","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nval_prediction = bert_model.predict([val_ids,val_masks])\nval_pred = np.argmax(val_prediction.logits, axis=1)\nprint(classification_report(val_label, val_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history.keys())\nprint(history.history['val_loss'])\nprint(history.history['val_accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentences = test.text.values\ntest_max = max([len(sen) for sen in test_sentences]) # getting maximum sentence length for paddings\ntest_ids,test_masks = do_encode(test_sentences,test_max) # encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generating output predictions for the input samples\nprediction = bert_model.predict([test_ids,test_masks])\npredicted_labels = np.argmax(prediction.logits, axis=1) #probability values from 0 to 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Accuracy with only removing links')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'],loc='upper left')\nplt.savefig('accuracy.pdf')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss with only removing links')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'],loc='upper left')\nplt.savefig('loss.pdf')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(prediction)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating submission dataframe and submission file\nsubmit = pd.DataFrame({'id':submission.id,'target':predicted_labels})\nsubmit.to_csv('submission_with_removing_links.csv',index=False)\nsubmit.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### References:","metadata":{}},{"cell_type":"markdown","source":"* https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c \n* \nhttps://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=6J-FYdx6nFE\n* https://www.kaggle.com/code/gazu468/all-about-bert-you-need-to-know/notebook\n* https://www.youtube.com/watch?v=zJW57aCBCTk\n* https://www.youtube.com/watch?v=x66kkDnbzi4\n* https://www.youtube.com/watch?v=Hnvb9b7a_Ps","metadata":{}}]}